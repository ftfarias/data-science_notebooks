{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import addutils.toc ; addutils.toc.js(ipy_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from addutils import css_notebook\n",
    "css_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before trying any ML technique it is always a good idea to visualize the available data from different point of view but when the dimensionality of the problem is higher, it's difficult to use some that kind of plots and it is necessary to do **dimenionality reduction**.\n",
    "\n",
    "We have already seen some simple visualization example made with scatter plot or scatter matrix on the tutorial ml01. In this lesson we're going furter by working with some of the mamy data projection techniques available in scikit-learn.\n",
    "\n",
    "In the first example we will consider the *digits* dataset in which we have many 8x8 grayscale images of handwritten digits. In other words, this dataset is made by samples with 64 features\n",
    "\n",
    "What we want to do is to obtain a descriptive 2D scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import bokeh.plotting as bk\n",
    "bk.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import addutils.imagegrid as ig\n",
    "from bokeh.palettes import Greys9\n",
    "\n",
    "print(\"Digits images: \", digits.images.shape[0])\n",
    "# print only the first 80 digits images\n",
    "num_imgs = 80\n",
    "fig = ig.imagegrid_figure(images=[ digits.images[i][::-1, :] for i in range(num_imgs) ],\n",
    "                          text=[ str(digits.target[i]) for i in range(num_imgs) ],\n",
    "                          figure_title=None, palette=Greys9[::-1],\n",
    "                          figure_plot_width=760, figure_plot_height=600,\n",
    "                          text_color='red', padding=0.1,\n",
    "                          grid_size=(10, 8))\n",
    "bk.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start our analysis with Principal Component Analysis (PCA): PCA seeks orthogonal linear combinations of the features which show the greatest variance. In this example we'll use `RandomizedPCA`, because it's faster for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import addutils.palette as pal\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from bokeh.models.sources import ColumnDataSource\n",
    "from bokeh.models.tools import HoverTool\n",
    "\n",
    "pca = decomposition.RandomizedPCA(copy=True, iterated_power=3, n_components=2, whiten=False)\n",
    "pca_proj = pca.fit_transform(digits.data)\n",
    "\n",
    "colors = list(map(pal.to_hex, sns.color_palette('Paired', 10)))\n",
    "\n",
    "pca_df = pd.DataFrame({'x': pca_proj[:,0],\n",
    "                   'y': pca_proj[:,1],\n",
    "                   'color': pal.linear_map(digits.target, colors),\n",
    "                   'target': digits.target})\n",
    "pca_src = ColumnDataSource(data=pca_df)\n",
    "\n",
    "\n",
    "fig = bk.figure(title='PCA Projection - digits dataset',\n",
    "                x_axis_label='c1', y_axis_label='c2',\n",
    "                plot_width=750, plot_height=560)\n",
    "fig.scatter(source=pca_src, x='x', y='y',\n",
    "            size=10, fill_alpha=0.9, line_alpha=0.5, line_color='black',\n",
    "            fill_color='color')\n",
    "\n",
    "hover_tool = HoverTool(tooltips=[(\"target\", \"@target\"),\n",
    "                                 (\"color\", \"$color[swatch]:color\")])\n",
    "fig.add_tools(hover_tool)\n",
    "\n",
    "bk.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice some structure in this data but it's still difficult to understand if it would be possible to effectively apply some classification algorithm since the different data clusters do not show enough separation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) identifies the combination of attributes that account for the most variance in the data.\n",
    "\n",
    "Linear Discriminant Analysis (LDA) instead tries to identify attributes that account for the most variance between classes. In particular, LDA, in contrast to PCA, is a supervised method, and can be used just when the class labels are available.\n",
    "\n",
    "Let's see an LDA on the same Dataset:\n",
    "\n",
    "**TODO -** Remove the warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Variables are collinear')\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "\n",
    "lda_proj = lda.fit(digits.data, digits.target).transform(digits.data)\n",
    "\n",
    "lda_df = pd.DataFrame({ 'x': lda_proj[:,0],\n",
    "                        'y': lda_proj[:,1],\n",
    "                        'color': pal.linear_map(digits.target, colors),\n",
    "                        'target': digits.target })\n",
    "lda_src = ColumnDataSource(data=lda_df)\n",
    "\n",
    "fig = bk.figure(title='LDA Projection - digits dataset', \n",
    "                x_axis_label='c1', y_axis_label='c2',\n",
    "                plot_width=750, plot_height=500)\n",
    "fig.scatter(source=lda_src, x='x', y='y',\n",
    "            size=8, line_color='black', line_alpha=0.5,\n",
    "            fill_color='color')\n",
    "\n",
    "hover_tool = HoverTool(tooltips=[(\"target\", \"@target\"),\n",
    "                                 (\"color\", \"$color[swatch]:color\")])\n",
    "fig.add_tools(hover_tool)\n",
    "\n",
    "bk.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous example we don't see a clear separation of the different clusters. This is because this specific dataset present some non-linear features that can not be separate by linear projections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Manifold Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main weakness of the linear techniques seen so far is that they cannot detect non-linear features.  A set of algorithms known as *Manifold Learning* have been developed to address this deficiency.\n",
    "\n",
    "`Isomap` is a global, nonlinear, nonparametric dimentional reduction algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', message='kneighbors*')\n",
    "\n",
    "from sklearn import manifold\n",
    "\n",
    "from bokeh.models import GlyphRenderer, Quad, Legend\n",
    "\n",
    "fig_grid = []\n",
    "for i in range(2):\n",
    "    row = []\n",
    "    for j in range(2):\n",
    "        iso = manifold.Isomap(n_neighbors=i+2, n_components=2)\n",
    "        proj = iso.fit_transform(digits.data)\n",
    "        \n",
    "        df = pd.DataFrame({ 'x': proj[:,0], 'y': proj[:,1],\n",
    "                            'color': pal.linear_map(digits.target, colors),\n",
    "                            'target': digits.target })\n",
    "        src = ColumnDataSource(data=df)\n",
    "        \n",
    "        fig = bk.figure(title=\"n_neighbors = %d\" % (i+2),\n",
    "                        title_text_font_size='12pt',\n",
    "                        plot_width=340, plot_height=300)\n",
    "        fig.scatter(source=src, x='x', y='y', fill_color='color',\n",
    "                    size=8, line_alpha=0.5, line_color='black')\n",
    "        \n",
    "        hover_tool = HoverTool(tooltips=[ (\"target\", \"@target\") ])\n",
    "        fig.add_tools(hover_tool)\n",
    "        \n",
    "        row.append(fig)\n",
    "    fig_grid.append(row)\n",
    "\n",
    "bk.show(bk.gridplot(fig_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These visualizations show us that there is hope: even a simple classifier should be able to adequately identify the members of the various classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Another example un a specific test dataset: the S-Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A canonical dataset used in Manifold learning is the *S-curve*, which we briefly saw in an earlier section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = datasets.make_s_curve(n_samples=1000)\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax = plt.axes(projection='3d')\n",
    "s = ax.scatter3D(X[:, 0], X[:, 1], X[:, 2], c=y, s=50, marker=\"o\", cmap='GnBu')\n",
    "fig.colorbar(s)\n",
    "ax.view_init(10, -60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a 2-dimensional dataset embedded in three dimensions, but it is embedded in such a way that PCA cannot discover the underlying data orientation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqcolors = list(map(pal.to_hex, sns.color_palette('GnBu', 50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_pca = decomposition.PCA(n_components=2).fit_transform(X)\n",
    "\n",
    "fig = bk.figure(title='PCA Projection - S-Curve',\n",
    "               x_axis_label='c1', y_axis_label='c2', \n",
    "               plot_width=750, plot_height=500)\n",
    "fig.scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "            size=10, line_color='black', line_alpha=0.5,\n",
    "                fill_color=pal.linear_map(y, seqcolors))\n",
    "bk.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manifold learning algorithms, however, available in the `sklearn.manifold` submodule, are able to recover the underlying 2-dimensional manifold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iso = manifold.Isomap(n_neighbors=25, n_components=2)\n",
    "X_iso = iso.fit_transform(X)\n",
    "fig = bk.figure(title='Isomap - S-Curve', x_axis_label='c1', y_axis_label='c2',\n",
    "                plot_width=750, plot_height=400)\n",
    "fig.scatter(X_iso[:, 0], X_iso[:, 1], size=8, alpha=0.8, line_color='black',\n",
    "            fill_color=pal.linear_map(y, seqcolors))\n",
    "bk.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LocallyLinearEmbedding` (LLE) is a local, nonlinear, nonparametric algorithm. The main idea behind the Local Algorithms is to make the local configurations of points in the low-dimensional space resemble the local configurations in the high-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lle = manifold.LocallyLinearEmbedding(n_neighbors=15, n_components=2, method='modified')\n",
    "X_lle = lle.fit_transform(X)\n",
    "\n",
    "fig = bk.figure(title='LocallyLinearEmbedding - S-Curve', x_axis_label='c1', y_axis_label='c2', \n",
    "                plot_width=750, plot_height=400)\n",
    "fig.scatter(X_lle[:, 0], X_lle[:, 1],\n",
    "            size=10, line_color='black', line_alpha=0.75,\n",
    "            fill_color=pal.linear_map(y, seqcolors))\n",
    "bk.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Some Tips on Manifold Learning practical use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make sure the same scale is used over all features. Because manifold learning methods are based on a nearest-neighbor search, the algorithm may perform poorly otherwise.\n",
    "* The reconstruction error can be used to choose the optimal output dimension. For a *d-dimensional* manifold embedded in a * D-dimensional* parameter space, the reconstruction error will decrease as n_components is increased until *n_components == d*.\n",
    "* Noisy data can “short-circuit” the manifold, in essence acting as a bridge between parts of the manifold that would otherwise be well-separated.\n",
    "* Certain input configurations can lead to singular weight matrices, for example when more than two points in the dataset are identical, or when the data is split into disjointed groups. In this case, solver='arpack' will fail to find the null space. The easiest way to address this is to use solver='dense', though it may be very slow depending on the number of input points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 t-SNE t-distribuited Stochastic Neighbor Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`t-SNE` give more importance to local distances and lower importance to non-local distances. In other words, it try to keep closer in the projected space the points that are closer in the original space while neglecting the others.\n",
    "\n",
    "Moreover `t-SNE` has a probabilistic way to find pairwise local distances: it converts each high-dimension similarity into the probability that one data point will pick one other data point as its neighbor. This make `t-SNE` almost insensitive to bad feature scaling.\n",
    "\n",
    "On th other side, the relative local nature of `t-SNE` makes it sensitive to the course of the dimensionality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsne = manifold.TSNE(n_components=2, n_iter=500)\n",
    "tsne_proj = tsne.fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({ 'x': tsne_proj[:,0],\n",
    "                    'y': tsne_proj[:,1],\n",
    "                    'color': pal.linear_map(digits.target, colors),\n",
    "                    'target': digits.target\n",
    "                  })\n",
    "src = ColumnDataSource(data=df)\n",
    "\n",
    "fig = bk.figure(title='t-SNE - digits dataset', x_axis_label='c1', y_axis_label='c2',\n",
    "                plot_width=750, plot_height=450)\n",
    "fig.scatter(source=src, x='x', y='y', fill_color='color',\n",
    "            size=8, line_color='black', line_alpha=0.50)\n",
    "\n",
    "hover_tool = HoverTool(tooltips=[('target', '@target')])\n",
    "fig.add_tools(hover_tool)\n",
    "\n",
    "bk.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Visit [www.add-for.com](<http://www.add-for.com/IT>) for more tutorials and updates.\n",
    "\n",
    "This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
